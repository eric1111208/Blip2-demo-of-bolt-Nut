# âœ… YOLO + BLIP-2 + LoRA + Webcam æ¨ç†è…³æœ¬ (for Colab A100)

!pip install -U ultralytics
!pip install transformers accelerate peft bitsandbytes

from PIL import Image
import torch
from ultralytics import YOLO
from transformers import Blip2Processor, Blip2ForConditionalGeneration
from peft import PeftModel, PeftConfig
import os
import cv2
import numpy as np

# ========== Google Drive æ›è¼‰ ==========
from google.colab import drive
drive.mount('/content/drive')

# ========== è¨­å®šè·¯å¾‘ ==========
yolo_model_path = "/content/drive/MyDrive/project-ai/yolo_model/best.pt"
blip_model_path = "/content/drive/MyDrive/project-ai/blip2_lora_output/checkpoint-60"

# ========== è¨­å®šè£ç½® ==========
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32

# ========== è¼‰å…¥æ¨¡å‹ ==========
print("ğŸ“¦ åŠ è¼‰ YOLOv8 æ¨¡å‹...")
yolo_model = YOLO(yolo_model_path)

print("ğŸ§  åŠ è¼‰ BLIP-2 + LoRA æ¨¡å‹...")
processor = Blip2Processor.from_pretrained("Salesforce/blip2-opt-2.7b", use_fast=True)
peft_config = PeftConfig.from_pretrained(blip_model_path)
base_model = Blip2ForConditionalGeneration.from_pretrained(
    peft_config.base_model_name_or_path,
    torch_dtype=torch_dtype,
).to(device)
model = PeftModel.from_pretrained(base_model, blip_model_path).to(device)

# ========== å•Ÿç”¨ Colab Webcam ==========
from IPython.display import display, Javascript
from google.colab.output import eval_js
from base64 import b64decode
from io import BytesIO

def take_photo(filename='photo.jpg', quality=0.8):
    js = Javascript('''
      async function takePhoto(quality) {
        const div = document.createElement('div');
        const capture = document.createElement('button');
        capture.textContent = 'ğŸ“· Capture';
        div.appendChild(capture);

        const video = document.createElement('video');
        video.style.display = 'block';
        const stream = await navigator.mediaDevices.getUserMedia({video: true});

        document.body.appendChild(div);
        div.appendChild(video);
        video.srcObject = stream;
        await video.play();

        google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);

        await new Promise((resolve) => capture.onclick = resolve);

        const canvas = document.createElement('canvas');
        canvas.width = video.videoWidth;
        canvas.height = video.videoHeight;
        canvas.getContext('2d').drawImage(video, 0, 0);
        stream.getTracks().forEach(track => track.stop());
        div.remove();

        return canvas.toDataURL('image/jpeg', quality);
      }
      takePhoto({quality: %s});
    ''' % quality)
    display(js)
    data = eval_js("takePhoto()")
    binary = b64decode(data.split(',')[1])
    with open(filename, 'wb') as f:
        f.write(binary)
    return filename

# ========== æ•æ‰ç…§ç‰‡ä¸¦é€²è¡Œæ¨ç† ==========
photo_path = take_photo()
image = Image.open(photo_path).convert("RGB")
results = yolo_model.predict(photo_path, conf=0.5, verbose=False)
boxes = results[0].boxes

if boxes is not None and len(boxes.cls) > 0:
    for i in range(len(boxes.cls)):
        x1, y1, x2, y2 = map(int, boxes.xyxy[i].tolist())
        crop = image.crop((x1, y1, x2, y2))

        try:
            inputs = processor(images=crop, return_tensors="pt").to(device)
            out = model.generate(**inputs, max_new_tokens=32)
            caption = processor.decode(out[0], skip_special_tokens=True)
        except Exception as e:
            caption = "âŒ æ¨ç†å¤±æ•—ï¼š" + str(e)

        print(f"ç‰©ä»¶ {i+1}: {caption}")
else:
    print("â— æœªåµæ¸¬åˆ°ç‰©ä»¶")
