<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Eric Kua | Project Showcase</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            background-color: #0d1117;
            color: #c9d1d9;
            padding: 20px;
            max-width: 800px;
            margin: auto;
        }
        a {
            color: #58a6ff;
            text-decoration: none;
        }
        a:hover {
            text-decoration: underline;
        }
        h1, h2 {
            color: #f0f6fc;
        }
        .section {
            margin-top: 30px;
        }
        /* ğŸ‘‡ æ–°å¢åœ–ç‰‡å±•ç¤ºçš„æ¨£å¼ */
        .project {
            display: flex;
            align-items: flex-start;
            gap: 20px;
            margin-bottom: 30px;
        }
        .project img {
            width: 120px;
            height: 180px; /* é«˜åº¦è®Šå…©å€ï¼Œå¯è¦–æƒ…æ³èª¿æ•´ */
            border: 1px solid #ccc;
            background-color: #e0e0e0;
        }
        .project-text {
            flex: 1;
        }
        .project-text a {
            font-size: 2.5em; /* ğŸ”¥æ¯”h2å¤§ */
            line-height: 1.0;      /* ç¸®çŸ­è¡Œè· */
             color: #58a6ff;          /* æ”¹ç‚ºè—è‰² */
            font-weight: bold;
            display: inline-block;
            margin-bottom: 4px;
        }
        
        .project-text p {
            font-size: 1.0em;        /* èˆ‡ Skills å€ç›¸åŒå­—é«”å¤§å° */
            line-height: 1.1;      /* ç¸®çŸ­è¡Œè· */
            color: #dddddd;
            margin-top: 4px;
            margin-bottom: 0;
        }
        .project-item a {
            font-size: 2em;         /* å’Œ .project-text a ä¸€æ¨£å¤§ */
            color: #58a6ff;         /* è—è‰² */
            font-weight: bold;
            display: block;
            margin: 6px 0;
            text-decoration: none;
        }
        
        .project-item a:hover {
            text-decoration: underline;
            color: #79c0ff;
        }
    </style>
</head>
<body>
    <h1>Hi, I'm Eric Kua</h1>
    <p>A passionate engineer exploring AI, vision systems, and automation tools.</p>

    <div class="section">
        <h2>ğŸ› ï¸ Skills</h2>
        <ul>
            <li>Python, YOLO, BLIP-2, OpenCV, PyTorch</li>
            <li>PLC Programming (Mitsubishi, Siemens), Vision Inspection</li>
            <li>Data Annotation Tools, JSONL Management, UI (Tkinter, PyQt5)</li>
        </ul>
    </div>

    <div class="section">
        <h2>ğŸ“ Projects that combine YOLO for visual detection with BLIP and BLIP-2 for multi-modal reasoning tasks./h2>

        <!--(1) âœ… æ–°å¢åœ–æ–‡å±•ç¤ºå°ˆæ¡ˆ ===================== --->
        <div class="project">
            <a href="https://github.com/eric1111208/Blip2-demo-of-bolt-Nut#-blip-2-bolt--nut-demo" target="_blank">
                <img src="https://raw.githubusercontent.com/eric1111208/Blip2-demo-of-bolt-Nut/main/Screenshot from 2025-07-18 15-47-23.png" alt="BLIP-2 Demo" />
            </a>
            <div class="project-text">
                <a href="https://github.com/eric1111208/Blip2-demo-of-bolt-Nut#-blip-2-bolt--nut-demo" target="_blank"><strong>BLIP-2 Bolt & Nut Demo</strong></a>
                <p>
                    é€é JavaScript æ‹æ”ä¸€å¼µç…§ç‰‡ï¼ˆColab é™åˆ¶ï¼‰ã€‚<br>
                    ä½¿ç”¨ YOLO åµæ¸¬ç…§ç‰‡ä¸­çš„ç‰©ä»¶ï¼Œå°æ¯å€‹ç‰©ä»¶è£åˆ‡å€åŸŸï¼Œä½¿ç”¨ BLIP-2 + LoRA é€²è¡Œèªç¾©æè¿°ï¼Œåœ¨åœ–ç‰‡ä¸Šæ¨™è¨»ç‰©ä»¶æ¡†èˆ‡æ–‡å­—èªªæ˜ã€‚<br>
                    é¡¯ç¤ºä¸¦ç­‰å¾…2ç§’ï¼Œé€²å…¥ä¸‹ä¸€è¼ªæ‹ç…§èˆ‡æ¨ç†ã€‚
                </p>
            </div>
        </div>
        <!-- (2)===================== -->
        <div class="project">
            <a href="https://youtu.be/Ldvo9J-WMW0" target="_blank">
                <img src="https://raw.githubusercontent.com/eric1111208/Blip2-demo-of-bolt-Nut/main/DD2.png" alt="BLIP-2 Demo" />
            </a>
            <div class="project-text">
                <a href="https://youtu.be/Ldvo9J-WMW0" target="_blank"><strong>Video of yolo-Blip-model-demo-of-pencil-box</strong></a>
                <p>
                    This is a demo of the BLIP model trained on a laptop with a 4GB GPU.
                    A webcam runs a YOLO model for object detection. When a hand approaches,
                    it captures images, and the BLIP model outputs object relationships as text.
                    For example: "blue pen / red pen is taken out," "put in," "beside," "on top of," 
                    or "under the pencil box."
                </p>
            </div>
        </div>
        <!-- (3)===================== -->
        <div class="project">

           <a href="https://youtu.be/rdf6RCMR304" target="_blank">
                <img src="https://raw.githubusercontent.com/eric1111208/Blip2-demo-of-bolt-Nut/main/DD3.png" alt="BLIP-2 Demo" />
            </a>
            <div class="project-text">
                <a href="https://youtu.be/rdf6RCMR304" target="_blank"><strong>Image Annotation UI--code and operation</strong></a>
                <p>
                    Images Browsing ,Bbox labelling and Classification tool<br>
                    Develop by using PyQt5 ui library, write in python code <br>
                </p>
            </div>
        </div>
        <!-- =(4)==================== -->
        <div class="project">
            <a href="https://youtu.be/d1ENpsYS_5w" target="_blank">
                <img src="https://raw.githubusercontent.com/eric1111208/Blip2-demo-of-bolt-Nut/main/DD4.png" alt="BLIP-2 Demo" />
            </a>
            <div class="project-text">
                <a href="https://youtu.be/d1ENpsYS_5w" target="_blank"><strong>YOLOv8 + ArUco Marker: Real-Time Screw Size Detection (M8/M10) with mm Precision</strong></a>
                <p>
                    ğŸ” "Capture real-time video from a camera, use an ArUco marker to estimate<br>
                    the scale (pixels per millimeter), then use a YOLOv8 model <br>
                    to detect objects in the frame. For each detected object, <br>
                    it calculates the real-world size in millimeters, <br>
                    determines if it matches an M8 or M10 screw size, <br>
                    and overlays the classification, confidence, size (in mm), <br>
                    and label (e.g., M8/M10) on the image."<br>
                </p>
            </div>
        </div>
        <!-- (5)===================== -->
        <div class="project">
            <a href="https://https://github.com/eric1111208/Project-history/blog/main/README.md" target="_blank">
                <img src="https://raw.githubusercontent.com/eric1111208/Blip2-demo-of-bolt-Nut/main/Screenshot from 2025-07-18 15-47-23.png" alt="Project History" />
            </a>
            <div class="project-text">
                <a href="https://github.com/eric1111208/Project-history/blog/main/README.md" target="_blank"><strong>Project History</strong></a>
                <p>
                   Project History
                </p>
            </div>
        </div>
        <!-- ===================== -->
    </div>

    <div class="section">
        <h2>ğŸ“š Currently Learning</h2>
        <ul>
            <li>LoRA Fine-Tuning</li>
            <li>Scene Graph to GPT Reasoning</li>
            <li>Cloud TPU Training on Google Colab</li>
        </ul>
    </div>

    <div class="section">
        <h2>ğŸ“¬ Contact Me</h2>
        <ul>
            <li>Email: <a href="mailto:erickua208@gmail.com">erickua208@gmail.com</a></li>
          
        </ul>
    </div>
</body>
</html>

